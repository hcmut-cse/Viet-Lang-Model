{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nyTRawcs2Kw"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8-YLkGWsrz3"
   },
   "outputs": [],
   "source": [
    "HOME_PATH = './'\n",
    "\n",
    "WEIGHT_PATH = os.path.join(HOME_PATH, 'weight')\n",
    "if not os.path.isdir(WEIGHT_PATH):\n",
    "    os.makedirs(WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sE8Ev65DsWf1"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYgDqN9EsWf2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8kCA5TNsWf5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtGgpfK1sWf8"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embeding_size, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.embeding = nn.Embedding(vocab_size, embeding_size)\n",
    "        self.lstm = nn.LSTM(embeding_size, hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        # x: BxS\n",
    "        x = self.embeding(x) # BxSxE\n",
    "        if hidden_state is None:\n",
    "            x, hidden_state = self.lstm(x) # BxSx2H\n",
    "        else:\n",
    "            x, hidden_state = self.lstm(x, hidden_state) # BxSx2H\n",
    "        x = F.relu(x)\n",
    "        x = self.linear(x) # BxSxV\n",
    "        return x, hidden_state\n",
    "    \n",
    "    def predict(self, x, hidden_state=None):\n",
    "        x, hidden_state = self.forward(x, hidden_state)\n",
    "        x = F.softmax(x, dim=-1) # BxSxV\n",
    "        return x, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(os.path.join(WEIGHT_PATH, 'vocab.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ku2DBA6DsWgG"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 200\n",
    "hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDPgqgWSsWgI"
   },
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eMeQgxDVzxRL",
    "outputId": "904932fa-d5fe-4526-d2ac-04a4d0821e15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embeding): Embedding(90, 200)\n",
       "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (linear): Linear(in_features=256, out_features=90, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_param = torch.load(os.path.join(WEIGHT_PATH, 'model.h5'))\n",
    "model.load_state_dict(weight_param)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNgNlDYDdsed"
   },
   "outputs": [],
   "source": [
    "def update_topk(topk_seq, topk_score, new_hs, new_cs, probs, k, ix):\n",
    "    # topk_seq: kxS\n",
    "    # topk_score: kx1\n",
    "    # topk_hs, topk_cs: ?xkx?\n",
    "    # probs: kx1xV\n",
    "    topk_probs, topk_ix = probs[:,-1,:].topk(k)\n",
    "    topk_log = topk_probs.log()\n",
    "    new_scores = topk_log + topk_score\n",
    "    \n",
    "    k_probs, k_ix = new_scores.view(-1).topk(k)\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "    \n",
    "    topk_seq[:, :ix] = topk_seq[row, :ix]\n",
    "    topk_seq[:, ix] = topk_ix[row, col]\n",
    "    \n",
    "    topk_hs = new_hs[:,row,:]\n",
    "    topk_cs = new_cs[:,row,:]\n",
    "    \n",
    "    topk_score = k_probs.unsqueeze(1)\n",
    "    return topk_seq, topk_score, topk_hs, topk_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om-ETDZWCn1H"
   },
   "outputs": [],
   "source": [
    "def beam_search(start, model, k=3, max_len=30, device=-1):\n",
    "    seq = tokenizer.texts_to_sequences([start]) # 1xS\n",
    "    seq = torch.tensor(seq).long()\n",
    "    topk_seq = torch.zeros(k, max_len).long()\n",
    "#     topk_seq[:,:len(start)] = seq[0,:]\n",
    "    prob, (hs, cs) = model.predict(seq) # 1xSxV, hs\n",
    "    prob_k, idx_k = prob[:,-1,:].topk(k=k, dim=-1)\n",
    "#     cix = len(start)\n",
    "    cix = 0\n",
    "    topk_seq[:,cix] = idx_k[0]\n",
    "    cix+=1\n",
    "    topk_score = torch.zeros(k, 1)\n",
    "    topk_hs = torch.zeros(hs.size(0), k, hs.size(2))\n",
    "    topk_cs = torch.zeros(cs.size(0), k, cs.size(2))\n",
    "    topk_hs[:,:,:] = hs[:,-1,:].unsqueeze(1)\n",
    "    topk_cs[:,:,:] = cs[:,-1,:].unsqueeze(1)\n",
    "    \n",
    "    res_seq = []\n",
    "    res_score = []\n",
    "    \n",
    "    for i in range(cix, max_len):\n",
    "        probs, (hs, cs) = model.predict(topk_seq[:,i-1].unsqueeze(-1), (topk_hs, topk_cs)) # 1xSxV, hs\n",
    "        topk_seq, topk_score, topk_hs, topk_cs = update_topk(topk_seq, topk_score, hs, cs, probs, k, i)\n",
    "\n",
    "        eos_tok = tokenizer.word_index[' ']\n",
    "        eos_ix = (topk_seq==eos_tok).nonzero()\n",
    "        \n",
    "        seq_end_ix = eos_ix[:,0].numpy()\n",
    "        seq_notend_ix = [t for t in range(k) if t not in seq_end_ix]\n",
    "        \n",
    "        if len(seq_end_ix) > 0:\n",
    "            _seq = topk_seq[seq_end_ix]\n",
    "            for s in _seq:\n",
    "                res_seq.append(s.numpy())\n",
    "            \n",
    "            _score = topk_score[seq_end_ix]\n",
    "            for s in _score:\n",
    "                res_score.append(s.numpy()[0])\n",
    "            \n",
    "            topk_score = topk_score[seq_notend_ix].contiguous()\n",
    "            topk_seq = topk_seq[seq_notend_ix].contiguous()\n",
    "            topk_hs = topk_hs[:,seq_notend_ix,:].contiguous()\n",
    "            topk_cs = topk_cs[:,seq_notend_ix,:].contiguous()\n",
    "            \n",
    "            k -= len(seq_end_ix)\n",
    "            \n",
    "        if k==0:\n",
    "            break\n",
    "       \n",
    "    \n",
    "    return res_seq, res_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4rrj18POcK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^nguyễn trà \n",
      "^nguyễn trí \n",
      "^nguyễn trần \n",
      "^nguyễn trờng \n",
      "^nguyễn trọng \n",
      "^nguyễn trình \n",
      "^nguyễn trung \n",
      "^nguyễn trường \n",
      "^nguyễn trungng \n",
      "^nguyễn trrường \n"
     ]
    }
   ],
   "source": [
    "start = '^nguyễn tr'\n",
    "with torch.no_grad():\n",
    "    \n",
    "    seq, sc = beam_search(start, model, k=10)\n",
    "\n",
    "for s in convert_to_text(seq):print(start+s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "s3dJBY-G8LGR",
    "outputId": "83622b4c-6266-4c67-8fbf-44bbc3d62c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ần \n",
      "ờng \n",
      "ọng \n",
      "ường \n",
      "ungng \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.008477796, -0.7744576, -0.027882854, -0.24207884, -1.5635265]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwHnruMziTCs"
   },
   "outputs": [],
   "source": [
    "def seq2text(seq):\n",
    "    text = ''\n",
    "    for s in seq:\n",
    "        if s==0:\n",
    "            break\n",
    "        text += tokenizer.index_word[s]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDdmiRSLkTIy"
   },
   "outputs": [],
   "source": [
    "def convert_to_text(seqs):\n",
    "    res = []\n",
    "    for s in seqs:\n",
    "        t = seq2text(s)\n",
    "        res.append(t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9lQBfebkFVF"
   },
   "outputs": [],
   "source": [
    "def simple(start='^', max_len=25, end_char=' '):\n",
    "    seq = tokenizer.texts_to_sequences([start])\n",
    "    seq = torch.tensor(seq)\n",
    "    prob, hs = model.predict(seq)\n",
    "    name = start\n",
    "    ix = prob[:,-1,:].argmax(dim=-1).item()\n",
    "    name += tokenizer.index_word[ix]\n",
    "    end_token = tokenizer.word_index[end_char]\n",
    "    for i in range(len(start), max_len):\n",
    "        seq = torch.tensor([[ix]])\n",
    "        prob, hs = model.predict(seq, hs)\n",
    "        ix = prob[:,-1,:].argmax(dim=-1).item()\n",
    "        if ix == end_token:\n",
    "            break\n",
    "        name += tokenizer.index_word[ix]\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMFMze-nKNVK"
   },
   "outputs": [],
   "source": [
    "def gen_name():\n",
    "    name = ''\n",
    "    seq = np.zeros((1,20,1), dtype=np.float)\n",
    "    pred = model.predict(seq)\n",
    "    ix = np.random.choice(list(range(NUM_CHAR)), p=pred.ravel())\n",
    "    name = name + ix_to_char[ix]\n",
    "    while len(name) < 50:\n",
    "        seq = preprocess(name)\n",
    "        pred = model.predict(seq)\n",
    "        ix = np.random.choice(list(range(NUM_CHAR)), p=pred.ravel())\n",
    "        name = name + ix_to_char[ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "    return name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3y0gJTVYrD25",
    "outputId": "7167b658-5e9a-482e-8153-b1927b86c1dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^nguyễn văn'"
      ]
     },
     "execution_count": 105,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple('^nguyễn vă')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdZ3Nl07l4Mo"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(inp, k=5):\n",
    "    seq = tokenizer.texts_to_sequences([inp])\n",
    "    seq = torch.tensor(seq)\n",
    "    prob, hs = model.predict(seq)\n",
    "    kprob, kix = prob[0,-1,:].topk(k)\n",
    "    res = {tokenizer.index_word[i]:p for p,i in zip(kprob.detach().numpy(), kix.detach().numpy())}\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxnYwkc-9QRW"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(s, k=10):\n",
    "    seq = tokenizer.texts_to_sequences([s])\n",
    "    seq = torch.tensor(seq)\n",
    "    prob, hs = model.predict(seq)\n",
    "    kprob, kix = prob[0,-1,:].topk(k)\n",
    "    for p, i in zip(kprob.detach().numpy(), kix.detach().numpy()):\n",
    "        print(tokenizer.index_word[i], '%.4f'%(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "0NJ9YWdtPn-b",
    "outputId": "60d1d271-7b2c-4eb5-9db7-af85d8e70041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 0.6883\n",
      "u 0.0304\n",
      "t 0.0278\n",
      "m 0.0231\n",
      "d 0.0207\n",
      "h 0.0201\n",
      "p 0.0184\n",
      "y 0.0183\n",
      "l 0.0156\n",
      "o 0.0123\n"
     ]
    }
   ],
   "source": [
    "predict_next_char('^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjGt2kG-PqTH"
   },
   "outputs": [],
   "source": [
    "VOCAB_PATH = os.path.join(HOME_PATH, 'vocab.h5')\n",
    "\n",
    "torch.save(tokenizer, VOCAB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkyHHs5xvPth"
   },
   "outputs": [],
   "source": [
    "state = torch.load(os.path.join(WEIGHT_PATH, 'epoch_99.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qZvUSq3e0k4_",
    "outputId": "185320b0-dd58-46e4-c087-f15d08cb0ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EkCBdoXq153a",
    "outputId": "a2f26775-dc6f-4f30-b41f-f51ccb90d433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optim'])"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMsMVcQf0qTI"
   },
   "outputs": [],
   "source": [
    "t = state['model']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "name_generation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
